---
title: "Raport - Zaawansowane metody klasyfikacji oraz analiza skupień – algorytmy grupujące i hierarchiczne"
author: "Filip Michewicz 282239  \n  Wiktor Niedźwiedzki 258882"
date: "18 czerwca 2025 Anno Domini"
output:
  pdf_document:
    number_sections: true
toc: true
lof: true
lot: true
header-includes:
  - \usepackage[T1]{fontenc}
  - \usepackage[utf8]{inputenc}
  - \usepackage[polish]{babel}
  - \usepackage{graphicx}
  - \usepackage{float}
  - \renewcommand{\contentsname}{Spis treści}
  - \renewcommand{\listfigurename}{Spis wykresów}
  - \renewcommand{\listtablename}{Spis tabel}
  - \renewcommand{\figurename}{Wykres}
  - \renewcommand{\tablename}{Tabela}
  - \usepackage{xcolor}
  - \definecolor{ForestGreen}{rgb}{0.1333, 0.5451, 0.1333}
  - \definecolor{SteelBlue}{rgb}{0.2745, 0.5098, 0.7059}
  - \definecolor{Tomato}{rgb}{1.0, 0.3882, 0.2784}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  dpi = 1200,
  message = FALSE,
  warning = FALSE,
  error = TRUE,
  eval = TRUE,
  echo = FALSE,
  fig.align="center",
  fig.width = 7,
  fig.height = 4,
  fig.pos = "H",
  out.extra = '')
```

```{r biblioteki+zmienne_pomocnicze}
library(adabag)
library(ipred)
library(rpart)
library(rpart.plot)
library(mlbench)
library(randomForest)
library(HDclassif)
library(e1071)
library(knitr)
library(kableExtra)
library(stats)
library(clue)
library(cluster)
library(factoextra)
library(clValid)

start <- Sys.time()

data(wine)
data(Glass)
Glass.copy <- Glass

set.seed(21370)
```

```{r metryki}
metryki <- function(conf) {
  TP <- diag(conf)
  FP <- colSums(conf) - TP
  FN <- rowSums(conf) - TP
  TN <- sum(conf) - (TP + FP + FN)
  
  precision <- ifelse((TP + FP) == 0, 0, TP / (TP + FP))
  recall <- ifelse((TP + FN) == 0, 0, TP / (TP + FN))
  f1 <- ifelse((precision + recall) == 0, 0, 
               2 * precision * recall / (precision + recall))
  
  accuracy <- sum(TP) / sum(conf)
  accuracy_per_class <- (TP + TN) / (TP + TN + FP + FN)
  
  results <- data.frame(
    Accuracy = accuracy_per_class,
    Precision = precision,
    Recall = recall,
    F1_Score = f1
  )
  
  avg_metrics <- c(
    Accuracy = mean(accuracy_per_class), 
    Precision = mean(precision), 
    Recall = mean(recall), 
    F1_Score = mean(f1)
  )
  
  results <- rbind(results, avg_metrics)
  rownames(results)[nrow(results)] <- "Średnie"
  
  return(results)
}
```

\newpage

# Zaawansowane metody klasyfikacji

W pierwszej części zadania zastosujemy algorytmy *ensemble learning* (bagging,
boosting i random forest) w celu poprawy dokładności cech klasyfikacyjnych. 
W drugiej natomiast poznamy i ocenimy nową metodę klasyfikacji - metodę wektorów nośnych (SVM).

Zadanie zostanie wykonane na zbiorze danych *wine*, którego szczegółowy opis znajduje się w poprzednim raporcie.

## Rodziny klasyfikatorów/uczenie zespołowe

Wyróżniamy trzy algorytmy uczenia zespołowego (ang. ensemble learning):

* **Bagging** - generujemy B-bootstrapowych replikacji zbioru uczącego, na podstawie których tworzymy B klasyfikatorów. Następnie łączymy je w klasyfikator zagregowany, który przydziela dane cechy do klas za pomocą reguły "głosowania większości" (w przypadku remisu wybiera losowo). Każdy klasyfikator powstaje niezależnie (w sensie takim, że wyniki poprzednich nie mają wpływu na generowanie nowych).
* **Boosting** - podobnie jak w bagging, tworzymy klasyfikator zagregowany złożony z wielu pojedynczych klasyfikatorów. Jednak różnica jest taka, że klasyfikatory powstają sekwencyjnie. Na początku każda cecha w zbiorze ma przypisaną taką samą wagę. Z każdą kolejną iteracją natomiast waga zwiększa się dla uprzednio źle sklasyfikowanych przypadków.
* **Random forest** (dla drzew klasyfikacyjnych) - metoda podobna do bagging z tą różnicą, że klasyfikatory powstają na podstawie różnych m-elementowych podzbiorach cech (m mniejsze bądź równe wszystkim cechom).

```{r wczytanie_win}
colnames(wine) <- c(
  "Gatunek", "Alkohol", "Kwas_mlekowy", "Popiół", "Zasadowość_popiołu", 
  "Magnez", "Fenole_ogółem", "Flawonoidy", 
  "Fenole_nieflawonoidowe", "Proantocyjanidyny",
  "Koloru", "Barwa", "OD280_OD315", "Prolina"
)
wine$Gatunek <- as.factor(wine$Gatunek)
wine$Magnez <- as.numeric(wine$Magnez)
wine$Prolina <- as.numeric(wine$Prolina)

col <- sapply(wine, is.numeric)
wine_scaled <- wine
wine_scaled[col] <- scale(wine[col])
```

Na wykresie przedstawiono pojedyńcze drzewo klasyfikacyjne.

```{r drzewo_klasyfikacyjne, fig.cap="Pojedyńcze drzewo klasyfikacyjne", cache = TRUE}
mypredict.rpart <- function(object, newdata)  predict(object, newdata=newdata, type="class")

mypredict.boost <- function(object, newdata) as.factor(predict(object, newdata=newdata)$class)

est <- "632plus"
K = 10

tree_model <- rpart(Gatunek ~ ., data = wine, method = "class")
rpart.plot(tree_model, type = 2, extra = 104, fallen.leaves = TRUE)
error.tree <- (errorest(Gatunek ~ ., data = wine,
                       model = rpart,
                       predict = mypredict.rpart,
                       estimator = est,
                       est.para = control.errorest(nboot = K)))$error
```

Błąd estymowany metodą bootstrap .632+ wynosi `r round(error.tree*100, 2)`%. Na jego podstawie określimy poprawę modelu po zastosowaniu metod uczenia zespołowego. 

**MOŻESZ DAĆ TU WZÓR ŻE POPRAWĘ DEFINIUJEMY JAKO (błąd jednego drzewa - błąd metody)/ błąd jednego drzewa * 100% w Latechuj**

W tej części analizy do oceny wydajności modelu zostanie wykorzystana wyłącznie metoda .632+, ponieważ koryguje ona obciążenie estymatora (bias) i dostarcza bardziej wiarygodnej oceny błędu generalizacji, zwłaszcza przy ryzyku przeuczenia i ograniczonej liczbie próbek.


``` {r drzewa-klasyfikacyjne, cache = TRUE}
mypredict.rpart <- function(object, newdata)  predict(object, newdata=newdata, type="class")

mypredict.boost <- function(object, newdata) as.factor(predict(object, newdata=newdata)$class)

lista <- list()
B.vector <- c(1, 5, 10, 20, 30, 40, 50, 100)

error.bagging <- 0
error.randomForest <- 0
error.boosting <- 0

for(b in B.vector){
  
  error.bagging <- (errorest(Gatunek~., data = wine, model = bagging,
                                            estimator = est,
                                            est.para = control.errorest(nboot = K),
                                            nbagg = b))$error
    
  error.randomForest <- (errorest(Gatunek~., data = wine, model = randomForest,
                                                      estimator = est,
                                                      est.para = control.errorest(nboot = K),
                                                      model.args = list(ntree = b)))$error
    
  error.boosting <- (errorest(Gatunek~., data = wine, model = boosting,
                                              predict = mypredict.boost,
                                              estimator = est,
                                              est.para = control.errorest(nboot = K),
                                              model.args = list(mfinal = b)))$error
  
  poprawa <- c((error.tree - error.bagging)/error.tree*100,
               (error.tree - error.randomForest)/error.tree*100,
              (error.tree - error.boosting)/error.tree*100)
  
  lista[[as.character(b)]] <- poprawa

}

matrix <- do.call(cbind, lista)
rownames(matrix) <- c("Bagging", "Random Forest", "Boosting")
```

```{r}
kable(matrix,
  caption = "Poprawa dokładności klasyfikacji za pomocą drzewa klasyfikacyjnego, z podziałem na algorytmy uczenia zespołowego oraz liczbę replikacji", 
  digits = 2) %>%
  add_header_above(c("Algorytm \n uczenia \n zespołowego" = 1, "Liczba replikacji" = 8)) %>% 
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

## Metoda wektorów nośnych (SVM)

W tej części przeprowadzona będzie klasyfikacja na podstawie metody wektorów nośnych, z podziałem na różne funkcje jądrowe.

Metoda SVM jest jedną z najczęściej stosowanych technik uczenia maszynowego w zadaniach klasyfikacyjnych. Jej podstawowym celem jest wyznaczenie hiperpłaszczyzny maksymalnie oddzielającej obserwacje należące do różnych klas, przy jednoczesnym maksymalizowaniu marginesu między klasami.

Dzięki zastosowaniu funkcji jądrowych (kernel functions), SVM umożliwia również skuteczną klasyfikację danych nieliniowo separowalnych poprzez odwzorowanie ich do przestrzeni o wyższej liczbie wymiarów. W niniejszej analizie zostaną porównane różne funkcje jądrowe, w tym liniowa, wielomianowa oraz radialna (RBF), w kontekście ich wpływu na jakość klasyfikacji.

```{r super-uber-duper-ultra-zajebiste-funkcje, cache = TRUE}

C <- 10^(-3:3) #kara
gamma <- c(0.01, 0.1, 1:10)

#NIE działają dla kernel = "linear"

#Wielokrotny podział
kernel.type <- function(K = 10, size = 2/3,
                        scale = FALSE, kernel){
  
  df <- as.data.frame(matrix(NA, nrow = length(C), ncol = length(gamma)))
  
  rownames(df) <- as.character(C)
  colnames(df) <- as.character(gamma)
  
  for(c in C){
    
    for(g in gamma){
      
      accuracy <- 0
      
      for(i in 1:K){
        
        id.learn <- sample(seq_len(nrow(wine)),
                           size = floor(nrow(wine) * size))
        
        if(scale==TRUE){
          train.set <- wine_scaled[id.learn, ]
          test.set <- wine_scaled[-id.learn, ]
        }
        else{
          train.set <- wine[id.learn, ]
          test.set <- wine[-id.learn, ]
        }
        
        svm <- svm(Gatunek ~ ., data=train.set, kernel = kernel,
                   gamma = g, cost = c)
        
        prediction <- predict(svm, newdata=test.set)
        prediction <- table(prediction, test.set$Gatunek)
        
        accuracy <- accuracy + sum(diag(prediction))/sum(prediction)
      }
      
      df[as.character(c), as.character(g)] <- accuracy/K*100
    
    }
  }
  
  return(df)
}

#Cross-validation
kernel.type.cv <- function(K = 10, scale = FALSE, kernel){
  
  df <- as.data.frame(matrix(NA, nrow = length(C), ncol = length(gamma)))
  
  rownames(df) <- as.character(C)
  colnames(df) <- as.character(gamma)
  
  folds <- sample(rep(seq_len(K), length.out = nrow(wine)))
  
  for(c in C){
    
    for(g in gamma){
      
      accuracy <- 0
      
      for(i in 1:K){
        
        if(scale == TRUE){
          train.set <- wine_scaled[folds != i, ]
          test.set <- wine_scaled[folds == i, ]
        }
        else{
          train.set <- wine[folds != i, ]
          test.set <- wine[folds == i, ]
        }
        
        svm <- svm(Gatunek ~ ., data=train.set, kernel = kernel,
                   gamma = g, cost = c)
        
        prediction <- predict(svm, newdata=test.set)
        prediction <- table(prediction, test.set$Gatunek)
        
        accuracy <- accuracy + sum(diag(prediction))/sum(prediction)
      }
      
      df[as.character(c), as.character(g)] <- accuracy/K*100
    
    }
  }
  
  return(df)
}

#Bootstrap
kernel.type.bs <- function(K = 10, size = 2/3, scale = FALSE, kernel){
  
  df <- as.data.frame(matrix(NA, nrow = length(C), ncol = length(gamma)))
  
  rownames(df) <- as.character(C)
  colnames(df) <- as.character(gamma)
  
  for(c in C){
    
    for(g in gamma){
      
      accuracy <- 0
      
      for(i in 1:K){
        
        id.learn <- sample(seq_len(nrow(wine)),
                           size = floor(nrow(wine) * size),
                           replace = TRUE)
        
        if(scale == TRUE){
          train.set <- wine_scaled[id.learn, ]
          test.set <- wine_scaled[-id.learn, ]
        }
        else{
          train.set <- wine[id.learn, ]
          test.set <- wine[-id.learn, ]
        }
        
        svm <- svm(Gatunek ~ ., data=train.set, kernel = kernel,
                   gamma = g, cost = c)
        
        prediction <- predict(svm, newdata=test.set)
        prediction <- table(prediction, test.set$Gatunek)
        
        accuracy <- accuracy + sum(diag(prediction))/sum(prediction)
      }
      
      df[as.character(c), as.character(g)] <- accuracy/K*100
    
    }
  }
  
  return(df)
}

```

### Jądro liniowe

Przeanalizowano skuteczność klasyfikacyjną z zastosowaniem **jądra liniowego**, zarówno **bez skalowania danych**, jak i **po ich skalowaniu**.

Porównanie wyników dla obu wariantów (ze skalowaniem i bez) pozwala ocenić wpływ przeskalowania zmiennych na jakość klasyfikacji. Ponieważ SVM opiera się na obliczeniach odległości i iloczynów skalarnych, skalowanie danych może znacząco wpłynąć na działanie algorytmu, szczególnie gdy zmienne wejściowe różnią się skalą lub jednostką.

Za każdym razem skuteczność modelu oceniano trzema metodami: **wielokrotnego podziału**, **bootstrapu** oraz **kroswalidacji**. W pierwszych dwóch przypadkach stosowano podział danych w stosunku **2:1** (czyli 2/3 zbioru do nauki, 1/3 do testowania), natomiast **kroswalidację** przeprowadzono z użyciem **10 zbiorów** (10-fold), przy czym model trenowano na 9 częściach, a testowano na jednej. Wszystkie podawane wartości dokładności odnoszą się do skuteczności modelu na zbiorze testowym.

```{r, cache = TRUE}

df <- matrix(NA, nrow = 3, ncol = length(C))

#Wielokrotny podział
col <- 0
for(c in C){
  
  col <- col + 1
  
  accuracy <- 0
  for(i in seq_len(K)){
    
    id.learn <- sample(seq_len(nrow(wine)), size = floor(nrow(wine) * 2/3))
    train.set <- wine[id.learn, ]
    test.set <- wine[-id.learn, ]
    
    svm <- svm(Gatunek ~ ., data=train.set, kernel = "linear", cost = c)
    
    prediction <- predict(svm, newdata=test.set)
    prediction <- table(prediction, test.set$Gatunek)
    
    accuracy <- accuracy + sum(diag(prediction))/sum(prediction)
  }
  
  df[1, col] <- accuracy/K*100
}

#Cross-validation
folds <- sample(rep(seq_len(K), length.out = nrow(wine)))
col <- 0

for(c in C){
  
  col <- col + 1
  
  accuracy <- 0
  for(i in seq_len(K)){
    
    train.set <- wine[folds == i, ]
    test.set <- wine[folds != i, ]
    
    svm <- svm(Gatunek ~ ., data=train.set, kernel = "linear", cost = c)
    
    prediction <- predict(svm, newdata=test.set)
    prediction <- table(prediction, test.set$Gatunek)
    
    accuracy <- accuracy + sum(diag(prediction))/sum(prediction)
  }
  
  df[2, col] <- accuracy/K*100
}

#Bootstrap
col <- 0

for(c in C){
  
  col <- col + 1
  
  accuracy <- 0
  for(i in seq_len(K)){
    
    id.learn <- sample(seq_len(nrow(wine)), size = floor(nrow(wine) * 2/3),
                       replace = TRUE)

    train.set <- wine[id.learn, ]
    test.set <- wine[-id.learn, ]
    
    svm <- svm(Gatunek ~ ., data=train.set, kernel = "linear", cost = c)
    
    prediction <- predict(svm, newdata=test.set)
    prediction <- table(prediction, test.set$Gatunek)
    
    accuracy <- accuracy + sum(diag(prediction))/sum(prediction)
  }
  
  df[3, col] <- accuracy/K*100
}

df <- rbind(df, colMeans(df))
df <- as.data.frame(df)
rownames(df) <- c("Wielokrotny podział",
                  "Cross-validation",
                  "Bootstrap",
                  "Średnio")
colnames(df) <- as.character(C)
```

```{r}
kable(df, caption = "Jądro liniowe - bez skalowania", digits = 2) %>%
  add_header_above(c("Metoda" = 1, "Współczynnik kary - C" = 7)) %>% 
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

```{r, cache = TRUE}

df <- matrix(NA, nrow = 3, ncol = length(C))

#Wielokrotny podział
col <- 0
for(c in C){
  
  col <- col + 1
  
  accuracy <- 0
  for(i in seq_len(K)){
    
    id.learn <- sample(seq_len(nrow(wine)), size = floor(nrow(wine) * 2/3))
    
    train.set <- wine_scaled[id.learn, ]
    test.set <- wine_scaled[-id.learn, ]
    
    svm <- svm(Gatunek ~ ., data=train.set, kernel = "linear", cost = c)
    
    prediction <- predict(svm, newdata=test.set)
    prediction <- table(prediction, test.set$Gatunek)
    
    accuracy <- accuracy + sum(diag(prediction))/sum(prediction)
  }
  
  df[1, col] <- accuracy/K*100
}

#Cross-validation
folds <- sample(rep(seq_len(K), length.out = nrow(wine)))
col <- 0

for(c in C){
  
  col <- col + 1
  
  accuracy <- 0
  for(i in seq_len(K)){
    
    train.set <- wine_scaled[folds == i, ]
    test.set <- wine_scaled[folds != i, ]
    
    svm <- svm(Gatunek ~ ., data=train.set, kernel = "linear", cost = c)
    
    prediction <- predict(svm, newdata=test.set)
    prediction <- table(prediction, test.set$Gatunek)
    
    accuracy <- accuracy + sum(diag(prediction))/sum(prediction)
  }
  
  df[2, col] <- accuracy/K*100
}

#Bootstrap
col <- 0

for(c in C){
  
  col <- col + 1
  
  accuracy <- 0
  for(i in seq_len(K)){
    
    id.learn <- sample(seq_len(nrow(wine)), size = floor(nrow(wine) * 2/3),
                       replace = TRUE)
    
    train.set <- wine_scaled[id.learn, ]
    test.set <- wine_scaled[-id.learn, ]
    
    svm <- svm(Gatunek ~ ., data=train.set, kernel = "linear", cost = c)
    
    prediction <- predict(svm, newdata=test.set)
    prediction <- table(prediction, test.set$Gatunek)
    
    accuracy <- accuracy + sum(diag(prediction))/sum(prediction)
  }
  
  df[3, col] <- accuracy/K*100
}

df <- rbind(df, colMeans(df))
df <- as.data.frame(df)
rownames(df) <- c("Wielokrotny podział",
                  "Cross-validation",
                  "Bootstrap",
                  "Średnio")
colnames(df) <- as.character(C)
```

```{r}
kable(df, caption = "Jądro liniowe - ze skalowaniem", digits = 2) %>%
  add_header_above(c("Metoda" = 1, "Współczynnik kary - C" = 7)) %>% 
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

Z analizy Tabeli 2. oraz Tabeli 3. wynika, że optymalną wartością współczynnika kary - **C** w klasyfikacji SVM z jądrem liniowym jest **0.1**. Dla tej wartości obserwuje się najwyższą średnią skuteczność klasyfikacji zarówno bez skalowania, jak i po skalowaniu danych. Ponadto, zastosowanie skalowania cech nieznacznie poprawia wyniki, co wskazuje na korzystny wpływ normalizacji na efektywność modelu.

Wyższe wartości parametru **C** nie przekładają się na istotną poprawę skuteczności klasyfikacji, a w niektórych przypadkach powodują nawet jej nieznaczny spadek. Wynika to z faktu, że zbyt duża wartość **C** powoduje nadmierne dopasowanie modelu do danych treningowych (overfitting). W konsekwencji, mimo że model stara się minimalizować błędy na zbiorze treningowym, jego efektywność na danych testowych nie ulega poprawie, co potwierdzają uzyskane wyniki.

### Jądro wielomianowe

Przeanalizowano skuteczność klasyfikacyjną z zastosowaniem **jądra wielomianowego**, zarówno **bez skalowania danych**, jak i **po ich skalowaniu**.

Podobnie jak w przypadku jądra liniowego, porównanie wyników dla obu wariantów pozwala ocenić wpływ przeskalowania zmiennych na jakość klasyfikacji. W przypadku jądra wielomianowego, oprócz parametru **C**, uwzględniany jest także dodatkowy parametr **gamma**, który wpływa na działanie funkcji jądrowej i może modyfikować złożoność granicy decyzyjnej. Skalowanie cech ma zatem istotne znaczenie również ze względu na większą liczbę parametrów wrażliwych na różnice w skali danych.


```{r, cache = TRUE}
kable(kernel.type(kernel="polynomial"), digits = 2,
      caption = "Jądro wielomianowe - wielokrotny podział, bez skalowania") %>%
  add_header_above(c("Współcznnik \n kary - C" = 1, "Współczynnik - gamma" = 12)) %>% 
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

```{r, cache = TRUE}
dataFrame <- kernel.type(scale = TRUE, kernel="polynomial")

kable(dataFrame, digits = 2,
      caption = "Jądro wielomianowe - wielokrotny podział, ze skalowaniem") %>%
  add_header_above(c("Współcznnik \n kary - C" = 1, "Współczynnik - gamma" = 12)) %>% 
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

```{r, cache = TRUE}
kable(kernel.type.cv(kernel="polynomial"), digits = 2,
      caption = "Jądro wielomianowe - cross-validation, bez skalowania") %>%
  add_header_above(c("Współcznnik \n kary - C" = 1, "Współczynnik - gamma" = 12)) %>% 
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

W Tabeli 6. widać efekt plateau - dla wielu kombinacji wartości **C** i **gamma** dokładność utrzymuje się na poziomie 96,11%. Oznacza to, że w badanym zakresie hiperparametrów model osiągnął nasycenie: dalsze zwiększanie **C** czy **gamma** nie poprawia dopasowania ani dokładności. Problem plateau utrudnia dalszą optymalizację, ponieważ w “płaskim obszarze” przestrzeni parametrów zmiana wartości nie przynosi korzyści.

```{r, cache = TRUE}
kable(kernel.type.cv(scale = TRUE, kernel="polynomial"), digits = 2,
      caption = "Jądro wielomianowe - cross-validation, ze skalowaniem") %>%
  add_header_above(c("Współcznnik \n kary - C" = 1, "Współczynnik - gamma" = 12)) %>% 
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

```{r, cache = TRUE}
kable(kernel.type.bs(kernel="polynomial"), digits = 2,
      caption = "Jądro wielomianowe - bootstrap, bez skalowania") %>%
  add_header_above(c("Współcznnik \n kary - C" = 1, "Współczynnik - gamma" = 12)) %>% 
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

```{r, cache = TRUE}
kable(kernel.type.bs(scale = TRUE, kernel="polynomial"), digits = 2,
      caption = "Jądro wielomianowe - bootstrap, ze skalowaniem") %>%
  add_header_above(c("Współcznnik \n kary - C" = 1, "Współczynnik - gamma" = 12)) %>% 
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

Analiza Tabel 4–9 prowadzi do wniosku, że najwyższą dokładność klasyfikacji uzyskano dla kombinacji hiperparametrów: **gamma = 10** oraz **C = 0.1**. Ponadto zauważono, że zastosowanie skalowania cech miało korzystny wpływ na wyniki – w większości przypadków prowadziło do poprawy skuteczności klasyfikatora.

Dodatkowo przeanalizowano wpływ stopnia wielomianu jądra na jakość klasyfikacji.


``` {r, cache = TRUE}
degree <- 2:10
df <- as.data.frame(matrix(NA, nrow=1, ncol=length(degree)))
rownames(df) <- "Dokładność"
colnames(df) <- as.character(degree)

K <- 10
for(d in degree){
  
  accuracy <- 0
  
  for(i in seq_len(K)){
    
    id.learn <- sample(seq_len(nrow(wine)), size = floor(nrow(wine) * 2/3))
    train.set <- wine_scaled[id.learn, ]
    test.set <- wine_scaled[-id.learn, ]
    
    svm <- svm(Gatunek ~ ., data=train.set, kernel = "polynomial",
               degree = d, gamma = 10, cost = 0.1)
    
    prediction <- predict(svm, newdata=test.set)
    prediction <- table(prediction, test.set$Gatunek)
    
    accuracy <- accuracy + sum(diag(prediction))/sum(prediction)
  
  }
  
  df[1, as.character(d)] <- accuracy/K*100

}
```

```{r}
kable(df, digits = 2,
      caption = "Dokładność klasyfikacji w zależności od stopnia wielomianu — metoda wielokrotnego podziału z użyciem najlepszej kombinacji parametrów gamma i C") %>% 
  add_header_above(c(" " = 1, "Stopień wielomianu" = 9))
```

Z Tabeli 10. wynika, że najwyższą dokładność klasyfikacji uzyskano dla wielomianu stopnia **3** – czyli tego samego, który został zastosowany przy wcześniejszym doborze optymalnych wartości parametrów **gamma** i **C**. Potwierdza to trafność wyboru tego stopnia jako podstawy do dalszej optymalizacji modelu.

### Jądro radialne (RBF)

Przeanalizowano skuteczność klasyfikacyjną z zastosowaniem jądra radialnego, zarówno **bez skalowania danych**, jak i **po ich skalowaniu**.

Podobnie jak w przypadku jąder liniowego i wielomianowego, porównanie wyników dla obu wariantów umożliwia ocenę wpływu przeskalowania zmiennych na jakość klasyfikacji. W przypadku jądra radialnego, oprócz parametru **C**, kluczową rolę odgrywa również parametr **gamma**, który kontroluje zasięg wpływu pojedynczych obserwacji treningowych.

```{r, cache = TRUE}
kable(kernel.type.cv(kernel="radial"), digits = 2,
      caption = "Jądro radialne - wielokrotny podział, bez skalowania") %>%
  add_header_above(c("Współcznnik \n kary - C" = 1, "Współczynnik - gamma" = 12)) %>% 
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

```{r, cache = TRUE}
kable(kernel.type.cv(scale = TRUE, kernel="radial"), digits = 2,
      caption = "Jądro radialne - wielokrotny podział, ze skalowaniem") %>%
  add_header_above(c("Współcznnik \n kary - C" = 1, "Współczynnik - gamma" = 12)) %>% 
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

```{r, cache = TRUE}
kable(kernel.type.cv(kernel="radial"), digits = 2,
      caption = "Jądro radialne - cross-validation, bez skalowania") %>%
  add_header_above(c("Współcznnik \n kary - C" = 1, "Współczynnik - gamma" = 12)) %>% 
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

```{r, cache = TRUE}
kable(kernel.type.cv(scale = TRUE, kernel="radial"), digits = 2,
      caption = "Jądro radialne - cross-validation, ze skalowaniem") %>%
  add_header_above(c("Współcznnik \n kary - C" = 1, "Współczynnik - gamma" = 12)) %>% 
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

W Tabelach 11–14. obserwujemy, że zarówno dla danych bez skalowania, jak i po ich skalowaniu, dokładność utrzymuje się na poziomie około 39,9% dla wielu kombinacji parametrów **gamma** i **C**. Odwołująć się do Raportu 3. "Gdybyśmy przypisali wszystkie obserwacje do najczęściej występującej klasy, uzyskalibyśmy dokładność na poziomie 39.89%."

Oznacza to, że model w tych ustawieniach przypisuje wszystkie obserwacje do dominującej klasy, co wskazuje na underfitting. 

Fakt, że skalowanie nie poprawiło wyniku, sugeruje, iż problem nie wynika wyłącznie z różnic w skali cech, lecz także z nieoptymalnego zakresu hiperparametrów, możliwej nierównowagi klas lub niewystarczającej reprezentacji cech do separacji. Efekt plateau w obu wariantach (ze skalowaniem i bez) oznacza, że w badanym zakresie dalsze zmiany **gamma** i **C** nie wpływają na poprawę dokładności, ponieważ model nie „widzi” struktur rozróżniających klasy.

```{r, cache = TRUE}
kable(kernel.type.bs(kernel="radial"), digits = 2,
      caption = "Jądro radialne - bootstrap, bez skalowania") %>%
  add_header_above(c("Współcznnik \n kary - C" = 1, "Współczynnik - gamma" = 12)) %>% 
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

```{r, cache = TRUE}
kable(kernel.type.bs(scale= TRUE, kernel="radial"), digits = 2,
      caption = "Jądro radialne - bootstrap, ze skalowaniem") %>%
  add_header_above(c("Współcznnik \n kary - C" = 1, "Współczynnik - gamma" = 12)) %>% 
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

Tabela 15. i 16. nie wykazują efektu plateau, jednak uzyskane wyniki są niskie. Zaobserwowano, że najlepsze rezultaty pojawiają się przy niskich wartościach **gamma** i wysokim współczynniku kary **C**, co sugeruje, że taka konfiguracja sprzyja lepszej separacji klas. W celu dalszej poprawy dokładności przeprowadzono dodatkowe testy z użyciem metody bootstrap po skalowaniu danych, modyfikując zakres parametrów **gamma** i **C**.

```{r radialne-poprawa}
  C <- 10^(0:6) #kara
  gamma <- 10^(-6:0)

#Bootstrap
kernel.type.bs.better <- function(K = 10, size = 2/3, scale = FALSE, kernel){
  
  df <- as.data.frame(matrix(NA, nrow = length(C), ncol = length(gamma)))
  
  rownames(df) <- as.character(C)
  colnames(df) <- as.character(gamma)
  
  for(c in C){
    
    for(g in gamma){
      
      accuracy <- 0
      
      for(i in 1:K){
        
        id.learn <- sample(seq_len(nrow(wine)),
                           size = floor(nrow(wine) * size),
                           replace = TRUE)
        
        if(scale == TRUE){
          train.set <- wine_scaled[id.learn, ]
          test.set <- wine_scaled[-id.learn, ]
        }
        else{
          train.set <- wine[id.learn, ]
          test.set <- wine[-id.learn, ]
        }
        
        svm <- svm(Gatunek ~ ., data=train.set, kernel = kernel,
                   gamma = g, cost = c)
        
        prediction <- predict(svm, newdata=test.set)
        prediction <- table(prediction, test.set$Gatunek)
        
        accuracy <- accuracy + sum(diag(prediction))/sum(prediction)
      }
      
      df[as.character(c), as.character(g)] <- accuracy/K*100
    
    }
  }
  
  return(df)
}
```

```{r, cache = TRUE}
kable(kernel.type.bs.better(scale= TRUE, kernel="radial"), digits = 2,
      caption = "Jądro radialne - bootstrap, ze skalowaniem - zmienione parametry C i gamma") %>%
  add_header_above(c("Współcznnik \n kary - C" = 1, "Współczynnik - gamma" = 7)) %>% 
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

Analiza Tabeli 17. pokazuje, że bardzo niskie wartości **gamma** i **C** skutkują niską dokładnością na poziomie już spotkanego przypisania wszystkich obserwacji do najliczniejszej klasy. Bardzo duże **C** wymuszają surową karę za błędy, co zwiększa dopasowanie do danych, ale może prowadzić do przeuczenia. Natomiast małe **gamma** powoduje, że wpływ pojedynczych punktów jest szeroki, co wygładza granicę decyzyjną i zapobiega nadmiernemu dopasowaniu. Połączenie dużego **C** z małym lub umiarkowanym **gamma** daje najlepsze wyniki, zapewniając równowagę między dopasowaniem a uogólnianiem modelu.

Na podstawie tego stwierdzamy że najlepsze wyniki dla jądra radialnego osiągane są dla **C** wynoszącego 1 000 000 i **gamma** wynoszącego 0.0001.

### Jądro sigmoidalne

Przeanalizowano skuteczność klasyfikacyjną z zastosowaniem jądra sigmoidalnego, zarówno **bez skalowania danych**, jak i **po ich skalowaniu**.

Podobnie jak w przypadku jąder liniowego, wielomianowego i radialnego, porównanie wyników dla obu wariantów umożliwia ocenę wpływu przeskalowania zmiennych na jakość klasyfikacji.

```{r, cache = TRUE}
C <- 10^(-1:5) #kara
gamma <- 10^(-3:3)

kable(kernel.type(kernel="sigmoid"), digits = 2,
      caption = "Jądro sigmoidalne - wielokrotny podział, bez skalowania") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

Z Tabeli 18. można wyciągnąć interesującą obserwację. Dla niskich wartości parametrów **C** oraz **gamma** klasyfikator SVM wykazuje bardzo niską skuteczność – na tyle niską, że jego działanie jest gorsze niż proste przypisanie wszystkich obserwacji do najczęściej występującej klasy w zbiorze danych.

Oznacza to, że przy zbyt małej karze za błąd klasyfikacji (**C**) oraz zbyt małym zasięgu wpływu pojedynczych obserwacji (**gamma**), model nie jest w stanie uchwycić żadnych istotnych wzorców w danych. W rezultacie uzyskana reguła klasyfikacyjna staje się praktycznie bezużyteczna i wykazuje się efektywnością niższą niż przypisanie wszystkich przypadków do jednej, dominującej klasy.

```{r, cache = TRUE}
kable(kernel.type(scale = TRUE, kernel="sigmoid"), digits = 2,
      caption = "Jądro sigmoidalne - wielokrotny podział, ze skalowaniem") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

```{r, cache = TRUE}
kable(kernel.type.cv(kernel="sigmoid"), digits = 2,
      caption = "Jądro sigmoidalne - cross-validation, bez skalowania") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

```{r, cache = TRUE}
kable(kernel.type.cv(scale = TRUE, kernel="sigmoid"), digits = 2,
      caption = "Jądro sigmoidalne - cross-validation, ze skalowaniem") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

```{r, cache = TRUE}
kable(kernel.type.bs(kernel="sigmoid"), digits = 2,
      caption = "Jądro sigmoidalne - bootstrap, bez skalowania") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

```{r, cache = TRUE}
kable(kernel.type.bs(scale = TRUE, kernel="sigmoid"), digits = 2,
      caption = "Jądro sigmoidalne - bootstrap, ze skalowaniem") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

Analizując wyniki przedstawione w Tabelach 18–23 dla jądra sigmoidalnego, można zauważyć, że model osiąga najlepsze wyniki klasyfikacyjne przy wartościach parametru **C** mieszczących się w przedziale od 100 do 1000 oraz **gamma** w zakresie od 0.001 do 0.01.

## Wnioski

e

\newpage

# Analiza skupień – algorytmy grupujące i hierarchiczne

W tym zadaniu zastosujemy i porównamy ze sobą metody analizy skupień - k-średnich i PAM jako algorytmy grupujące, oraz AGNES - algorytm hierarchiczny.

Zadanie zostanie wykonane na zbiorze danych *wine*, którego szczegółowy opis znajduje się w poprzednim raporcie.

To zadanie zostanie wykonane już na innym danych, którymi będzie zbiór *glass*.

## Charakterystyka danych

Zbiór danych glass zawiera **`r nrow(Glass)`** przypadków sześciu rodzajów szkła oraz **`r ncol(Glass)`** cech. Liczba brakujących
danych wynosi **`r sum(is.na(Glass))`**.

Znaczenie poszczególnych cech oraz ich typ przedstawiono w Tabeli 24.

```{r, cache=TRUE}
df_table <- data.frame(
  Typ = sapply(Glass, class),
  Opis = c("Współczynnik załamania światła",
           "Procent masowy azotu (%)",
           "Procent masowy magnezu (%)",
           "Procent masowy glinu (%)",
           "Procent masowy krzemu (%)",
           "Procent masowy potasu (%)",
           "Procent masowy wapnia (%)",
           "Procent masowy baru (%)",
           "Procent masowy żelaza (%)",
           "Klasa (typ szkła: 1, 2, 3, 5, 6, 7)"
           )
)

kable(df_table,
      col.names = c("Zmienna", "Typ", "Opis"),
      caption = "Opis zmiennych w zbiorze danych Glass") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")

```

```{r, cache=TRUE}
sumy <- rowSums(Glass[-c(1, 10)])
```

W poszczególnych przypadkach analizowanych próbek szkła sumy procentów masowych znajdują się w zakresie od `r min(sumy)` do `r max(sumy)`. Zaobserwowany nadmiar może wynikać z błędów zaokrągleń dokonanych przez autorów pomiarów. Natomiast niedomiar może być spowodowany zarówno obecnością innych pierwiastków chemicznych, nieuwzględnionych w zbiorze danych – tzw. pierwiastków śladowych – jak i niedokładnościami pomiarowymi lub zaokrągleniami dokonanymi przez autorów pomiarów.

```{r, cache=TRUE, fig.cap="Liczność poszczególnych typów szkła"}
etykiety <- Glass$Type

K <- length(unique(etykiety))
n <- length(etykiety)
p <- length(Glass) - 1

ggplot(data.frame(etykiety), aes(x = etykiety, fill = etykiety)) +
  geom_bar() +
  geom_text(stat = "count", aes(label = ..count..), vjust = -0.5) + 
  labs(x = "Gatunek wina", y = "Liczba wystąpień") +
  scale_fill_manual(values = c("tomato", "darkgoldenrod1", "darkgreen",
                               "steelblue","darkblue", "deeppink2")) +
  theme_minimal() +
  theme(legend.position = "none")
```

Z Wykresu 1. można odczytać, że liczba obserwacji poszczególnych klas w zbiorze danych jest bardzo zróżnicowana. Siedemdziesiąt obserwacji lub więcej posiadają typy 1. oraz 2. (co stanowi `r round((70+76)/214*100, 2)`% danych), reszta już po mniej niż 30 obserwacji.

```{r, fig.cap="Wykres pudełkowy, zmienne bez standaryzacji"}
boxplot(Glass[-10], las=2, col="tomato")
```

KURWA STANDARYZACJA MACHEN

```{r, fig.cap="Wykres pudełkowe, po standaryzacji"}
Glass[-10] <- scale(Glass[-10])
glass <- Glass[-10]

k <- length(levels(Glass$Type))

boxplot(glass, las=2, col="tomato") #Teraz zajebiście
```

Teraz jest zajebiście

```{r, fig.cap="Wizualizacja danych, PCA"}
pca_result <- prcomp(glass)
pca_summary <- summary(pca_result)$importance

xlab1 <- paste0("PC1 (", round(pca_summary[2, 1]*100, 2), "%)")
ylab1 <- paste0("PC2 (", round(pca_summary[2, 2]*100, 2), "%)")

plot(pca_result$x[, 1], pca_result$x[, 2],
     xlab = xlab1, ylab = ylab1,
     col = Glass$Type)
```

Chuja widać, ciekawe kto wybrał ten zbiór?

## Wyniki grupowania

Przeprowadzamy dla rzeczywistej liczby etykiet, która wynosi **`r k`**.

### k-średnie

```{r, fig.cap="PCA, kolory - rzeczywiste, kształt - wyniki"}
kmeans.k3 <- kmeans(glass, centers = k, iter.max = 10)
wyniki <- kmeans.k3$cluster

plot(pca_result$x[, 1], pca_result$x[, 2], xlab = xlab1, ylab = ylab1,
     col = Glass$Type, pch = wyniki)
```

Gównianie mu poszło

```{r, fig.cap="Wykres RI od Na, aby pokazać gdzie są wyznaczone centra skupień"}
plot(glass$RI, glass$Na, xlab = "RI", ylab = "Na",
     col = Glass$Type, pch = wyniki)
points(kmeans.k3$centers[,c("RI", "Na")], pch=16, cex=1.5, col=1:6)
```

Centra wywalone w kosmos, ale fajnie

```{r}
matrix <- table(wyniki, Glass$Type)

#Najlepsze dopasowanie
mapping <- solve_LSAP(matrix, maximum = TRUE)
wyniki1 <- mapping[wyniki]

matrix  <- table(wyniki1, Glass$Type)
rownames(matrix) <- as.character(c(1:3, 5:7))
colnames(matrix) <- as.character(c(1:3, 5:7))

kable(matrix , caption = "Macierz błędów; metoda k-średnich")
```

Dokładność (macierz): `r round(sum(diag(matrix))/sum(matrix)*100, 2)`%.

Dokładność (matchClasses, "exact"): `r round(compareMatchedClasses(wyniki, Glass$Type, method="exact")$diag*100, 2)`%.

### Partitioning Around Medoids (PAM)

```{r, fig.cap="coś"}
glass.conf_mat <- daisy(glass)

glass.pam7 <- pam(x=glass.conf_mat, diss=TRUE, k=k)

wyniki <- glass.pam7$clustering

Glass.copy$PAM <- wyniki

plot(pca_result$x[, 1], pca_result$x[, 2], xlab = xlab1, ylab = ylab1,
     col = wyniki, pch = as.numeric(Glass$Type))
```

Też słabo

```{r, fig.cap="coś, z medoidami"}
medoids <- glass[glass.pam7$id.med, ]

plot(glass$RI, glass$Na, xlab = "RI", ylab = "Na",
     col = wyniki, pch = as.numeric(Glass$Type))
points(medoids[,c("RI", "Na")], pch=16, cex=1.5, col=1:6)
```

Meh

```{r}
kable(Glass.copy[glass.pam7$id.med, ], caption="Dane medoidów, k=6")
```

```{r}
matrix <- table(wyniki, Glass$Type)

#Najlepsze dopasowanie
mapping <- solve_LSAP(matrix , maximum = TRUE)
wyniki1 <- mapping[wyniki]

conf_mat <- table(wyniki1, Glass$Type)
rownames(matrix) <- as.character(c(1:3, 5:7))
colnames(matrix) <- as.character(c(1:3, 5:7))

kable(matrix, caption = "Macierz błędów; metoda k-średnich")
```

Dokładność: `r round(sum(diag(matrix))/sum(matrix)*100, 2)`%.

Dokładność (matchClasses, "exact"): `r round(compareMatchedClasses(wyniki, Glass$Type, method="exact")$diag*100, 2)`%.

Gorzej niż k-średnie *shocked emoji*.

### Agglomerative Nesting (AGNES)

#### Najbliższy sąsiad

\newpage

```{r, fig.cap="AGNES: single linkage"}
glass.agnes.single <- agnes(x = glass.conf_mat,
                            diss = TRUE,
                            method = "single")

fviz_dend(glass.agnes.single, cex=0.4, k = k)
```

```{r}
wyniki <- cutree(glass.agnes.single, k = k)

matrix <- table(wyniki, Glass$Type)

#Najlepsze dopasowanie
mapping <- solve_LSAP(matrix , maximum = TRUE)
wyniki1 <- mapping[wyniki]

conf_mat <- table(wyniki1, Glass$Type)
rownames(matrix) <- as.character(c(1:3, 5:7))
colnames(matrix) <- as.character(c(1:3, 5:7))

kable(matrix, caption = "Macierz błędów; agnes, najbliższy sąsiad")
```

Dokładność: `r round(sum(diag(matrix))/sum(matrix)*100, 2)`%.

Dokładność (matchClasses, "exact"): `r round(compareMatchedClasses(wyniki, Glass$Type, method="exact")$diag*100, 2)`%.

#### Najdalszy sąsiad

\newpage

```{r, fig.cap="AGNES: complete linkage"}
glass.agnes.complete <- agnes(x = glass.conf_mat,
                            diss = TRUE,
                            method = "complete")

fviz_dend(glass.agnes.complete, cex=0.4, k = k)
```

```{r}
wyniki <- cutree(glass.agnes.complete, k = k)

matrix <- table(wyniki, Glass$Type)

#Najlepsze dopasowanie
mapping <- solve_LSAP(matrix , maximum = TRUE)
wyniki1 <- mapping[wyniki]

conf_mat <- table(wyniki1, Glass$Type)
rownames(matrix) <- as.character(c(1:3, 5:7))
colnames(matrix) <- as.character(c(1:3, 5:7))

kable(matrix, caption = "Macierz błędów; agnes, najdalszy sąsiad")
```

Dokładność: `r round(sum(diag(matrix))/sum(matrix)*100, 2)`%.

Dokładność (matchClasses, "exact"): `r round(compareMatchedClasses(wyniki, Glass$Type, method="exact")$diag*100, 2)`%.

#### Średnia odległość

\newpage

```{r, fig.cap="AGNES: average linkage"}
glass.agnes.average <- agnes(x = glass.conf_mat,
                            diss = TRUE,
                            method = "average")

fviz_dend(glass.agnes.average, cex=0.4, k = k)
```

```{r}
wyniki <- cutree(glass.agnes.average, k = k)

matrix <- table(wyniki, Glass$Type)

#Najlepsze dopasowanie
mapping <- solve_LSAP(matrix , maximum = TRUE)
wyniki1 <- mapping[wyniki]

conf_mat <- table(wyniki1, Glass$Type)
rownames(matrix) <- as.character(c(1:3, 5:7))
colnames(matrix) <- as.character(c(1:3, 5:7))

kable(matrix, caption = "Macierz błędów; agnes, średnia odległość")
```

Dokładność: `r round(sum(diag(matrix))/sum(matrix)*100, 2)`%.

Dokładność (matchClasses, "exact"): `r round(compareMatchedClasses(wyniki, Glass$Type, method="exact")$diag*100, 2)`%.

### Divisive clustering (DIANA)

#### Odległość euklidesowa

\newpage

```{r, fig.cap="AGNES: average linkage"}
glass.agnes.euclidean <- diana(x = glass.conf_mat,
                            diss = TRUE,
                            metric = "euclidean")

fviz_dend(glass.agnes.euclidean, cex=0.4, k = k)
```

```{r}
wyniki <- cutree(glass.agnes.euclidean, k = k)

matrix <- table(wyniki, Glass$Type)

#Najlepsze dopasowanie
mapping <- solve_LSAP(matrix , maximum = TRUE)
wyniki1 <- mapping[wyniki]

conf_mat <- table(wyniki1, Glass$Type)
rownames(matrix) <- as.character(c(1:3, 5:7))
colnames(matrix) <- as.character(c(1:3, 5:7))

kable(matrix, caption = "Macierz błędów; agnes, średnia odległość")
```

Dokładność: `r round(sum(diag(matrix))/sum(matrix)*100, 2)`%.

Dokładność (matchClasses, "exact"): `r round(compareMatchedClasses(wyniki, Glass$Type, method="exact")$diag*100, 2)`%.

#### Odległość Manhattan (taksówkowa)

\newpage

```{r, fig.cap="AGNES: average linkage"}
glass.agnes.manhattan <- diana(x = glass.conf_mat,
                            diss = TRUE,
                            metric = "manhattan")

fviz_dend(glass.agnes.manhattan, cex=0.4, k = k)
```

```{r}
wyniki <- cutree(glass.agnes.manhattan, k = k)

matrix <- table(wyniki, Glass$Type)

#Najlepsze dopasowanie
mapping <- solve_LSAP(matrix , maximum = TRUE)
wyniki1 <- mapping[wyniki]

conf_mat <- table(wyniki1, Glass$Type)
rownames(matrix) <- as.character(c(1:3, 5:7))
colnames(matrix) <- as.character(c(1:3, 5:7))

kable(matrix, caption = "Macierz błędów; agnes, średnia odległość")
```

Dokładność: `r round(sum(diag(matrix))/sum(matrix)*100, 2)`%.

Dokładność (matchClasses, "exact"): `r round(compareMatchedClasses(wyniki, Glass$Type, method="exact")$diag*100, 2)`%.

## Ocena jakości grupowania i wizualizacja najlepszych wyników

### Ocena

Legenda:

* 1 - kmeans,
* 2 - PAM,
* 3 - AGNES,
* 4 - DIANA.

```{r, fig.cap="coś", cache = TRUE}
metody <- c("kmeans", "pam", "agnes", "diana")
K.zakres <- 2:k

internal.validation <- clValid(glass,
                               nClust = K.zakres,
                               clMethods = metody,
                               validation = "internal")

measures <- internal.validation@measures

#Silhouette
silhouette <- data.frame("Kmeans" = measures[3, , 1],
                         "PAM" = measures[3, , 2],
                         "AGNES" = measures[3, , 3],
                         "DIANA" = measures[3, , 4],
                         "K" = 2:k)

#Connectivity
connectivity <- data.frame("Kmeans" = measures[1, , 1],
                           "PAM" = measures[1, , 2],
                           "AGNES" = measures[1, , 3],
                           "DIANA" = measures[1, , 4],
                           "K" = 2:k)

#Dunn
dunn <- data.frame("Kmeans" = measures[2, , 1],
                   "PAM" = measures[2, , 2],
                   "AGNES" = measures[2, , 3],
                   "DIANA" = measures[2, , 4],
                   "K" = 2:k)
```

```{r, fig.cap="Connectivity", cache = TRUE}
max <- max(connectivity[-5])

plot(connectivity$K, connectivity$Kmeans,
     type = "b", lty = 1, col = 1, lwd = 2,  pch=paste(1),
     xlab = "Liczba skupień", ylab = "Wskaźnik connectivity",
     ylim = c(0, max), las=1)

for(i in 2:4){
  lines(connectivity$K, as.vector(connectivity[, i]),
        type = "b", lty = i, col = i, lwd = 2, pch=paste(i))
}

legend("bottomright", legend = c("1", "2", "3", "4"), col = 1:4, lty = 1:4, pch = paste(1:4), lwd = 2)
```

Connectivity - im mniejszy, tym lepszy:

* kmeans - 2,
* PAM - 2,
* AGNES - 2 (najlepszy),
* DIANA - 3.

```{r, fig.cap="Dunn", cache = TRUE}
max <- max(dunn[-5])

plot(dunn$K, dunn$Kmeans,
     type = "b", lty = 1, col = 1, lwd = 2,  pch=paste(1),
     xlab = "Liczba skupień", ylab = "Wskaźnik Dunn'a",
     ylim = c(0, max), las=1)

for(i in 2:4){
  lines(dunn$K, as.vector(dunn[, i]),
        type = "b", lty = i, col = i, lwd = 2, pch=paste(i))
}

legend("bottomright", legend = c("1", "2", "3", "4"), col = 1:4, lty = 1:4, pch = paste(1:4), lwd = 2)
```

Dunn - im większy, tym lepszy:

* kmeans - 2,
* PAM - 2,
* AGNES - 2 (najlepszy),
* DIANA - 3.

```{r, fig.cap="Silhouette"}
plot(silhouette$K, silhouette$Kmeans,
     type = "b", lty = 1, col = 1, lwd = 2,  pch=paste(1),
     xlab = "Liczba skupień", ylab = "Indeks silhouette",
     ylim = c(0, 1), las=1)

for(i in 2:4){
  lines(silhouette$K, as.vector(silhouette[, i]),
        type = "b", lty = i, col = i, lwd = 2, pch=paste(i))
}

legend("bottomright", legend = c("1", "2", "3", "4"), col = 1:4, lty = 1:4, pch = paste(1:4), lwd = 2)
```

Dunn - im większy, tym lepszy:

* kmeans - 2,
* PAM - 2,
* AGNES - 2 (najlepszy),
* DIANA - 2.

### Wizualizacja

#### k-średnie

\newpage

```{r, fig.cap="PCA, kolory - rzeczywiste, kształt - wyniki, k=2"}
kmeans.k3 <- kmeans(glass, centers = 2, iter.max = 10)
wyniki <- kmeans.k3$cluster

plot(pca_result$x[, 1], pca_result$x[, 2], xlab = xlab1, ylab = ylab1,
     pch = wyniki+1, col = wyniki)
```

Gównianie mu poszło

```{r, fig.cap="Wykres RI od Na, aby pokazać gdzie są wyznaczone centra skupień, k=2"}
plot(glass$RI, glass$Na, xlab = "RI", ylab = "Na", pch = wyniki+1,
     col = wyniki)
points(kmeans.k3$centers[,c("RI", "Na")], pch = 16, cex = 1.5, col = 1:2)
```

Centra wywalone w kosmos, ale fajnie

Dokładność (matchedClasses): `r round(compareMatchedClasses(wyniki, Glass$Type)$diag*100, 2)`%.

#### PAM

\newpage

```{r, fig.cap="coś, k=2"}
glass.pam7 <- pam(x=glass.conf_mat, diss=TRUE, k=2)

wyniki <- glass.pam7$clustering

Glass.copy$PAM <- wyniki

plot(pca_result$x[, 1], pca_result$x[, 2], xlab = xlab1, ylab = ylab1,
     col = wyniki, pch = wyniki+1)
```

Też słabo

```{r, fig.cap="coś, z medoidami, k=2"}
medoids <- glass[glass.pam7$id.med, ]

plot(glass$RI, glass$Na, xlab = "RI", ylab = "Na",
     col = wyniki, pch = wyniki+1)
points(medoids[,c("RI", "Na")], pch = 16, cex = 1.5, col = 1:2)
```

Meh

Dokładność (matchedClasses): `r round(compareMatchedClasses(wyniki, Glass$Type)$diag*100, 2)`%.

```{r}
kable(Glass.copy[glass.pam7$id.med, ], caption="Dane medoidów, k=2")
```

#### AGNES

\newpage

```{r, fig.cap="AGNES: complete linage, k=2", cache = TRUE}
glass.agnes.complete <- agnes(x = glass.conf_mat,
                              diss = TRUE,
                              method = "complete")

fviz_dend(glass.agnes.complete, cex=0.4, k = 2)

wyniki <- cutree(glass.agnes.single, k = 2)
```

Tylko dla najdalszych sąsiadów, bo dał najlepsze wyniki.

Dokładność (matchedClasses): `r round(compareMatchedClasses(wyniki, Glass$Type)$diag*100, 2)`%.

#### DIANA

\newpage

```{r, fig.cap="DIANA, k=3", cache = TRUE}
glass.agnes.euclidean <- diana(x = glass.conf_mat,
                            diss = TRUE,
                            metric = "euclidean")

fviz_dend(glass.agnes.euclidean, cex=0.4, k = 3)

wyniki <- cutree(glass.agnes.euclidean, k = 3)
```

Dokładność (matchedClasses): `r round(compareMatchedClasses(wyniki, Glass$Type)$diag*100, 2)`%.

## Wnioski

# Podsumowanie

```{r, cache = TRUE}
end = Sys.time()
spend = end - start

total_seconds = as.numeric(spend, units = "secs")

minutes = floor(total_seconds / 60)
seconds = floor(total_seconds %% 60)
```

PS. Czas wykonywania kodu wynosi `r minutes` minut i `r seconds` sekund.