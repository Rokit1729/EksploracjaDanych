---
title: "Raport - Zaawansowane metody klasyfikacji oraz analiza skupień – algorytmy grupujące i hierarchiczne"
author: "Filip Michewicz 282239  \n  Wiktor Niedźwiedzki 258882"
date: "10 czerwca 2025 Anno Domini"
output:
  pdf_document:
    number_sections: true
toc: true
lof: true
lot: true
header-includes:
  - \usepackage[OT4]{polski}
  - \usepackage[utf8]{inputenc}
  - \usepackage{graphicx}
  - \usepackage{float}
  - \renewcommand{\contentsname}{Spis treści}
  - \renewcommand{\listfigurename}{Spis wykresów}
  - \renewcommand{\listtablename}{Spis tabel}
  - \renewcommand{\figurename}{Wykres}
  - \renewcommand{\tablename}{Tabela}
  - \usepackage{xcolor}
  - \definecolor{ForestGreen}{rgb}{0.1333, 0.5451, 0.1333}
  - \definecolor{SteelBlue}{rgb}{0.2745, 0.5098, 0.7059}
  - \definecolor{Tomato}{rgb}{1.0, 0.3882, 0.2784}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  dpi = 1200,
  message = FALSE,
  warning = FALSE,
  error = TRUE,
  eval = TRUE,
  echo = FALSE,
  fig.align="center",
  fig.width = 7,
  fig.height = 4,
  fig.pos = "H",
  out.extra = '')
```

```{r biblioteki+zmienne_pomocnicze}
library(adabag)
library(ipred)
library(rpart)
library(rpart.plot)
library(mlbench)
library(randomForest)
library(HDclassif)
library(e1071)
library(knitr)
library(kableExtra)
library(stats)
library(clue)
library(cluster)
library(factoextra)

start <- Sys.time()

data(wine)
data(Glass)

set.seed(213)
```

```{r metryki}
metryki <- function(conf) {
  TP <- diag(conf)
  FP <- colSums(conf) - TP
  FN <- rowSums(conf) - TP
  TN <- sum(conf) - (TP + FP + FN)
  
  precision <- ifelse((TP + FP) == 0, 0, TP / (TP + FP))
  recall <- ifelse((TP + FN) == 0, 0, TP / (TP + FN))
  f1 <- ifelse((precision + recall) == 0, 0, 
               2 * precision * recall / (precision + recall))
  
  accuracy <- sum(TP) / sum(conf)
  accuracy_per_class <- (TP + TN) / (TP + TN + FP + FN)
  
  results <- data.frame(
    Accuracy = accuracy_per_class,
    Precision = precision,
    Recall = recall,
    F1_Score = f1
  )
  
  avg_metrics <- c(
    Accuracy = mean(accuracy_per_class), 
    Precision = mean(precision), 
    Recall = mean(recall), 
    F1_Score = mean(f1)
  )
  
  results <- rbind(results, avg_metrics)
  rownames(results)[nrow(results)] <- "Średnie"
  
  return(results)
}
```

\newpage

# Zaawansowane metody klasyfikacji

W pierwszej części zadania zastosujemy algorytmy *ensemble learning* (bagging,
boosting i random forest) w celu poprawy dokładności cech klasyfikacyjnych. W drugiej natomiast poznamy i ocenimy nową metodę klasyfikacji - metodę wektorów nośnych (SVM).

Zadanie zostanie wykonane na zbiorze danych *wine*, którego szczegółowy opis znajduje się w poprzednim raporcie.

## Rodziny klasyfikatorów/uczenie zespołowe

Wyróżniamy trzy algorytmy uczenia zespołowego (ang. ensemble learning):

* **Bagging** - generujemy B-bootstrapowych replikacji zbioru uczącego, na podstawie których tworzymy B klasyfikatorów. Następnie łączymy je w klasyfikator zagregowany, który przydziela dane cechy do klas za pomocą reguły "głosowania większości" (w przypadku remisu wybiera losowo). Każdy klasyfikator powstaje niezależnie (w sensie takim, że wyniki poprzednich nie mają wpływu na generowanie nowych).
* **boosting** - podobnie jak w bagging, tworzymy klasyfikator zagregowany złożony z wielu pojedynczych klasyfikatorów. Jednak różnica jest taka, że klasyfikatory powstają sekwencyjnie. Na początku każda cecha w zbiorze ma przypisaną taką samą wagę. Z każdą kolejną iteracją natomiast waga zwiększa się dla uprzednio źle sklasyfikowanych przypadków.
* **random forest** (dla drzew klasyfikacyjnych) - metoda podobna do bagging z tą różnicą, że klasyfikatory powstają na podstawie różnych m-elementowych podzbiorach cech (m mniejsze bądź równe wszystkim cechom).

```{r}
colnames(wine) <- c(
  "Gatunek", "Alkohol", "Kwas_mlekowy", "Popiół", "Zasadowość_popiołu", 
  "Magnez", "Fenole_ogółem", "Flawonoidy", 
  "Fenole_nieflawonoidowe", "Proantocyjanidyny",
  "Koloru", "Barwa", "OD280_OD315", "Prolina"
)
wine$Gatunek <- as.factor(wine$Gatunek)
wine$Magnez <- as.numeric(wine$Magnez)
wine$Prolina <- as.numeric(wine$Prolina)

col <- sapply(wine, is.numeric)
wine_scaled <- wine
wine_scaled[col] <- scale(wine[col])
```

### Drzewa klasyfikacyjne

**MOŻE BYĆ NIEPOPRAWNIE W CHUJ**

```{r, cache = TRUE}
mypredict.rpart <- function(object, newdata)  predict(object, newdata=newdata, type="class")
mypredict.boost <- function(object, newdata) as.factor(predict(object, newdata=newdata)$class)

K <- 10
est <- "632plus"
lista <- list()
B.vector <- c(1, 5, 10, 20, 30, 40, 50, 100)

error.tree <- (errorest(Gatunek~., data = wine, model = rpart,
               predict = mypredict.rpart,
               estimator = est,
               est.para = control.errorest(nboot = K)))$error

error.bagging <- 0
error.randomForest <- 0
error.boosting <- 0

for(b in B.vector){
  
  error.bagging <- (errorest(Gatunek~., data = wine, model = bagging,
                                            estimator = est,
                                            est.para = control.errorest(nboot = K),
                                            nbagg = b))$error
    
  error.randomForest <- (errorest(Gatunek~., data = wine, model = randomForest,
                                                      estimator = est,
                                                      est.para = control.errorest(nboot = K),
                                                      model.args = list(ntree = b)))$error
    
  error.boosting <- (errorest(Gatunek~., data = wine, model = boosting,
                                              predict = mypredict.boost,
                                              estimator = est,
                                              est.para = control.errorest(nboot = K),
                                              model.args = list(mfinal = b)))$error
  
  poprawa <- c((error.tree - error.bagging)/error.tree*100,
               (error.tree - error.randomForest)/error.tree*100,
              (error.tree - error.boosting)/error.tree*100)
  
  lista[[as.character(b)]] <- poprawa

}

matrix <- do.call(cbind, lista)
matrix <- rbind(matrix, colSums(matrix)/3)
rownames(matrix) <- c("Bagging", "Random Forest", "Boosting", "Średnia")
```

```{r}
kable(matrix,
  caption = "Średnia poprawa dokładności klasyfikacji za pomocą drzewa klasyfikacyjnego, z podziałem na algorytmy uczenia zespołowego oraz liczbę replikacji", 
  digits = 2) %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

## Metoda wektorów nośnych (SVM)

W tej części przeprowadzona będzie klasyfikacja na podstawie metody wektorów nośnych, z podziałem na różne funkcje jądrowe.

**COŚ O TYM CO TO WOGÓLE JEST**

```{r super-uper-duper-ultra-zajebiste-funkcje, cache = TRUE}

C <- 10^(-3:3) #kara
gamma <- round(seq(from = .01, to = 10, length.out = 10), 2)

#NIE działają dla kernel = "linear"

#Wielokrotny podział
kernel.type <- function(K = 10, size = 2/3,
                        scale = FALSE, kernel){
  
  df <- as.data.frame(matrix(NA, nrow = length(C), ncol = length(gamma)))
  
  rownames(df) <- as.character(C)
  colnames(df) <- as.character(gamma)
  
  for(c in C){
    
    for(g in gamma){
      
      accuracy <- 0
      
      for(i in 1:K){
        
        id.learn <- sample(seq_len(nrow(wine)),
                           size = floor(nrow(wine) * size))
        
        if(scale==TRUE){
          train.set <- wine_scaled[id.learn, ]
          test.set <- wine_scaled[-id.learn, ]
        }
        else{
          train.set <- wine[id.learn, ]
          test.set <- wine[-id.learn, ]
        }
        
        svm <- svm(Gatunek ~ ., data=train.set, kernel = kernel,
                   gamma = g, cost = c)
        
        prediction <- predict(svm, newdata=test.set)
        prediction <- table(prediction, test.set$Gatunek)
        
        accuracy <- accuracy + sum(diag(prediction))/sum(prediction)
      }
      
      df[as.character(c), as.character(g)] <- accuracy/K*100
    
    }
  }
  
  return(df)
}

#Cross-validation
kernel.type.cv <- function(K = 10, scale = FALSE, kernel){
  
  df <- as.data.frame(matrix(NA, nrow = length(C), ncol = length(gamma)))
  
  rownames(df) <- as.character(C)
  colnames(df) <- as.character(gamma)
  
  folds <- sample(rep(seq_len(K), length.out = nrow(wine)))
  
  for(c in C){
    
    for(g in gamma){
      
      accuracy <- 0
      
      for(i in 1:K){
        
        if(scale == TRUE){
          train.set <- wine_scaled[folds != i, ]
          test.set <- wine_scaled[folds == i, ]
        }
        else{
          train.set <- wine[folds != i, ]
          test.set <- wine[folds == i, ]
        }
        
        svm <- svm(Gatunek ~ ., data=train.set, kernel = kernel,
                   gamma = g, cost = c)
        
        prediction <- predict(svm, newdata=test.set)
        prediction <- table(prediction, test.set$Gatunek)
        
        accuracy <- accuracy + sum(diag(prediction))/sum(prediction)
      }
      
      df[as.character(c), as.character(g)] <- accuracy/K*100
    
    }
  }
  
  return(df)
}

#Bootstrap
kernel.type.bs <- function(K = 10, size = 2/3, scale = FALSE, kernel){
  
  df <- as.data.frame(matrix(NA, nrow = length(C), ncol = length(gamma)))
  
  rownames(df) <- as.character(C)
  colnames(df) <- as.character(gamma)
  
  for(c in C){
    
    for(g in gamma){
      
      accuracy <- 0
      
      for(i in 1:K){
        
        id.learn <- sample(seq_len(nrow(wine)),
                           size = floor(nrow(wine) * size),
                           replace = TRUE)
        
        if(scale == TRUE){
          train.set <- wine_scaled[id.learn, ]
          test.set <- wine_scaled[-id.learn, ]
        }
        else{
          train.set <- wine[id.learn, ]
          test.set <- wine[-id.learn, ]
        }
        
        svm <- svm(Gatunek ~ ., data=train.set, kernel = kernel,
                   gamma = g, cost = c)
        
        prediction <- predict(svm, newdata=test.set)
        prediction <- table(prediction, test.set$Gatunek)
        
        accuracy <- accuracy + sum(diag(prediction))/sum(prediction)
      }
      
      df[as.character(c), as.character(g)] <- accuracy/K*100
    
    }
  }
  
  return(df)
}

```

### Jądro liniowe

```{r, cache = TRUE}

df <- matrix(NA, nrow = 3, ncol = length(C))

#Wielokrotny podział
col <- 0
for(c in C){
  
  col <- col + 1
  
  accuracy <- 0
  for(i in seq_len(K)){
    
    id.learn <- sample(seq_len(nrow(wine)), size = floor(nrow(wine) * 2/3))
    train.set <- wine[id.learn, ]
    test.set <- wine[-id.learn, ]
    
    svm <- svm(Gatunek ~ ., data=train.set, kernel = "linear", cost = c)
    
    prediction <- predict(svm, newdata=test.set)
    prediction <- table(prediction, test.set$Gatunek)
    
    accuracy <- accuracy + sum(diag(prediction))/sum(prediction)
  }
  
  df[1, col] <- accuracy/K*100
}

#Cross-validation
folds <- sample(rep(seq_len(K), length.out = nrow(wine)))
col <- 0

for(c in C){
  
  col <- col + 1
  
  accuracy <- 0
  for(i in seq_len(K)){
    
    train.set <- wine[folds == i, ]
    test.set <- wine[folds != i, ]
    
    svm <- svm(Gatunek ~ ., data=train.set, kernel = "linear", cost = c)
    
    prediction <- predict(svm, newdata=test.set)
    prediction <- table(prediction, test.set$Gatunek)
    
    accuracy <- accuracy + sum(diag(prediction))/sum(prediction)
  }
  
  df[2, col] <- accuracy/K*100
}

#Bootstrap
col <- 0

for(c in C){
  
  col <- col + 1
  
  accuracy <- 0
  for(i in seq_len(K)){
    
    id.learn <- sample(seq_len(nrow(wine)), size = floor(nrow(wine) * 2/3),
                       replace = TRUE)

    train.set <- wine[id.learn, ]
    test.set <- wine[-id.learn, ]
    
    svm <- svm(Gatunek ~ ., data=train.set, kernel = "linear", cost = c)
    
    prediction <- predict(svm, newdata=test.set)
    prediction <- table(prediction, test.set$Gatunek)
    
    accuracy <- accuracy + sum(diag(prediction))/sum(prediction)
  }
  
  df[3, col] <- accuracy/K*100
}

df <- rbind(df, colMeans(df))
df <- as.data.frame(df)
rownames(df) <- c("Wielokrotny podział",
                  "Cross-validation",
                  "Bootstrap",
                  "Średnio")
colnames(df) <- as.character(C)
```

```{r}
kable(df, caption = "Jądro liniowe - bez skalowania", digits = 2) %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

```{r, cache = TRUE}

df <- matrix(NA, nrow = 3, ncol = length(C))

#Wielokrotny podział
col <- 0
for(c in C){
  
  col <- col + 1
  
  accuracy <- 0
  for(i in seq_len(K)){
    
    id.learn <- sample(seq_len(nrow(wine)), size = floor(nrow(wine) * 2/3))
    
    train.set <- wine_scaled[id.learn, ]
    test.set <- wine_scaled[-id.learn, ]
    
    svm <- svm(Gatunek ~ ., data=train.set, kernel = "linear", cost = c)
    
    prediction <- predict(svm, newdata=test.set)
    prediction <- table(prediction, test.set$Gatunek)
    
    accuracy <- accuracy + sum(diag(prediction))/sum(prediction)
  }
  
  df[1, col] <- accuracy/K*100
}

#Cross-validation
folds <- sample(rep(seq_len(K), length.out = nrow(wine)))
col <- 0

for(c in C){
  
  col <- col + 1
  
  accuracy <- 0
  for(i in seq_len(K)){
    
    train.set <- wine_scaled[folds == i, ]
    test.set <- wine_scaled[folds != i, ]
    
    svm <- svm(Gatunek ~ ., data=train.set, kernel = "linear", cost = c)
    
    prediction <- predict(svm, newdata=test.set)
    prediction <- table(prediction, test.set$Gatunek)
    
    accuracy <- accuracy + sum(diag(prediction))/sum(prediction)
  }
  
  df[2, col] <- accuracy/K*100
}

#Bootstrap
col <- 0

for(c in C){
  
  col <- col + 1
  
  accuracy <- 0
  for(i in seq_len(K)){
    
    id.learn <- sample(seq_len(nrow(wine)), size = floor(nrow(wine) * 2/3),
                       replace = TRUE)
    
    train.set <- wine_scaled[id.learn, ]
    test.set <- wine_scaled[-id.learn, ]
    
    svm <- svm(Gatunek ~ ., data=train.set, kernel = "linear", cost = c)
    
    prediction <- predict(svm, newdata=test.set)
    prediction <- table(prediction, test.set$Gatunek)
    
    accuracy <- accuracy + sum(diag(prediction))/sum(prediction)
  }
  
  df[3, col] <- accuracy/K*100
}

df <- rbind(df, colMeans(df))
df <- as.data.frame(df)
rownames(df) <- c("Wielokrotny podział",
                  "Cross-validation",
                  "Bootstrap",
                  "Średnio")
colnames(df) <- as.character(C)
```

```{r}
kable(df, caption = "Jądro liniowe - ze skalowaniem", digits = 2) %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

### Jądro wielomianowe

```{r, cache = TRUE}
kable(kernel.type(kernel="polynomial"), digits = 2,
      caption = "Jądro wielomianowe - wielokrotny podział, bez skalowania") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

```{r, cache = TRUE}
dataFrame <- kernel.type(scale = TRUE, kernel="polynomial")

kable(dataFrame, digits = 2,
      caption = "Jądro wielomianowe - wielokrotny podział, ze skalowaniem") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

```{r, cache = TRUE}
kable(kernel.type.cv(kernel="polynomial"), digits = 2,
      caption = "Jądro wielomianowe - cross-validation, bez skalowania") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

```{r, cache = TRUE}
kable(kernel.type.cv(scale = TRUE, kernel="polynomial"), digits = 2,
      caption = "Jądro wielomianowe - cross-validation, ze skalowaniem") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

```{r, cache = TRUE}
kable(kernel.type.bs(kernel="polynomial"), digits = 2,
      caption = "Jądro wielomianowe - bootstrap, bez skalowania") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

```{r, cache = TRUE}
kable(kernel.type.bs(scale = TRUE, kernel="polynomial"), digits = 2,
      caption = "Jądro wielomianowe - bootstrap, ze skalowaniem") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

```{r, cache = TRUE}
best_gamma <- 0
best_C <- 0
kandydat <- 0

for(i in C){
  for(j in gamma){
    cell <- dataFrame[as.character(i), as.character(j)]
    if(cell > kandydat){
      best_gamma <- j
      best_C <- i
      kandydat <- cell
    }
  }
}

degree <- 2:7
df <- as.data.frame(matrix(NA, nrow=1, ncol=length(degree)))
rownames(df) <- "Dokładność"
colnames(df) <- as.character(degree)

K <- 10
for(d in degree){
  
  accuracy <- 0
  
  for(i in seq_len(K)){
    
    id.learn <- sample(seq_len(nrow(wine)), size = floor(nrow(wine) * 2/3))
    train.set <- wine_scaled[id.learn, ]
    test.set <- wine_scaled[-id.learn, ]
    
    svm <- svm(Gatunek ~ ., data=train.set, kernel = "polynomial",
               degree = d, gamma = best_gamma, cost = best_C)
    
    prediction <- predict(svm, newdata=test.set)
    prediction <- table(prediction, test.set$Gatunek)
    
    accuracy <- accuracy + sum(diag(prediction))/sum(prediction)
  
  }
  
  df[1, as.character(d)] <- accuracy/K*100

}
```

Najlepsza gamma: **`r best_gamma`**, najlepsza kara: **`r best_C`**. Robimy dla danych po standaryzacji, bo tak i chuj.

Badamy tylko na podstawie wielokrotnego podziału, bo tak i chuj również.

```{r}
kable(df, digits = 2,
      caption = "Badanie wpływu stopnia wielomianu na dokładność - wielokrotny podział, najbardziej dokładna kombinacja gammy i kary dla opcji default (stopień 3")
```

### Jądro radialne

```{r, cache = TRUE}
kable(kernel.type.cv(kernel="radial"), digits = 2,
      caption = "Jądro radialne - wielokrotny podział, bez skalowania") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

```{r, cache = TRUE}
kable(kernel.type.cv(scale = TRUE, kernel="radial"), digits = 2,
      caption = "Jądro radialne - wielokrotny podział, ze skalowaniem") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

```{r, cache = TRUE}
kable(kernel.type.cv(kernel="radial"), digits = 2,
      caption = "Jądro radialne - cross-validation, bez skalowania") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

```{r, cache = TRUE}
kable(kernel.type.cv(scale = TRUE, kernel="radial"), digits = 2,
      caption = "Jądro radialne - cross-validation, ze skalowaniem") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

```{r, cache = TRUE}
kable(kernel.type.bs(kernel="radial"), digits = 2,
      caption = "Jądro radialne - bootstrap, bez skalowania") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

```{r, cache = TRUE}
kable(kernel.type.bs(scale= TRUE, kernel="radial"), digits = 2,
      caption = "Jądro radialne - bootstrap, ze skalowaniem") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

### Jądro sigmoidalne

```{r, cache = TRUE}
kable(kernel.type(kernel="sigmoid"), digits = 2,
      caption = "Jądro sigmoidalne - wielokrotny podział, bez skalowania") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

```{r, cache = TRUE}
kable(kernel.type(scale = TRUE, kernel="sigmoid"), digits = 2,
      caption = "Jądro sigmoidalne - wielokrotny podział, ze skalowaniem") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

```{r, cache = TRUE}
kable(kernel.type.cv(kernel="sigmoid"), digits = 2,
      caption = "Jądro sigmoidalne - cross-validation, bez skalowania") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

```{r, cache = TRUE}
kable(kernel.type.cv(scale = TRUE, kernel="sigmoid"), digits = 2,
      caption = "Jądro sigmoidalne - cross-validation, ze skalowaniem") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

```{r, cache = TRUE}
kable(kernel.type.bs(kernel="sigmoid"), digits = 2,
      caption = "Jądro sigmoidalne - bootstrap, bez skalowania") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

```{r, cache = TRUE}
kable(kernel.type.bs(scale = TRUE, kernel="sigmoid"), digits = 2,
      caption = "Jądro sigmoidalne - bootstrap, ze skalowaniem") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), 
                repeat_header_text = "(kontynuacja)",
                stripe_color = "#B0B0B0")
```

elo

# Analiza skupień – algorytmy grupujące i hierarchiczne

W pierwszej części zadania zastosujemy algorytmy *ensemble learning* (bagging,
boosting i random forest) w celu poprawy dokładności cech klasyfikacyjnych. W drugiej natomiast poznamy i ocenimy nową metodę klasyfikacji - metodę wektorów nośnych (SVM).

Zadanie zostanie wykonane na zbiorze danych *wine*, którego szczegółowy opis znajduje się w poprzednim raporcie.

W tym zadaniu zastosujemy i porównamy ze sobą metody analizy skupień - k-średnich i PAM jako algorytmy grupujące, oraz AGNES - algorytm hierarhiczny.

## Zbiór danych Glass

Chujowy

```{r, fig.cap="Wykres pudełkowy, zmienne bez standaryzacji"}
boxplot(Glass[-10], las=2, col="tomato")
```

KURWA STANDARYZACJA MACHEN

```{r, fig.cap="Wykres pudełkowe, po standaryzacji"}
Glass[-10] <- scale(Glass[-10])
glass <- Glass[-10]

k <- length(levels(Glass$Type))

boxplot(glass, las=2, col="tomato") #Teraz zajebiście
```

Teraz jest zajebiście

```{r, fig.cap="Wizualizacja danych, PCA"}
pca_result <- prcomp(glass)
pca_summary <- summary(pca_result)$importance

xlab1 <- paste0("PC1 (", round(pca_summary[2, 1]*100, 2), "%)")
ylab1 <- paste0("PC2 (", round(pca_summary[2, 2]*100, 2), "%)")

plot(pca_result$x[, 1], pca_result$x[, 2],
     xlab = xlab1, ylab = ylab1,
     col = Glass$Type)
```

Chuja widać, ciekawe kto wybrał ten zbiór?

## Wyniki grupowania

Przeprowadzamy dla rzeczywistej liczby etykiet, która wynosi **`r k`**.

### k-średnie

```{r, fig.cap="PCA, kolory - rzeczywiste, kształt - wyniki"}
kmeans.k3 <- kmeans(glass, centers = k, iter.max = 10)
wyniki <- kmeans.k3$cluster

plot(pca_result$x[, 1], pca_result$x[, 2], xlab = xlab1, ylab = ylab1,
     col = Glass$Type, pch = wyniki)
```

Gównianie mu poszło

```{r, fig.cap="Wykres RI od Na, aby pokazać gdzie są wyznaczone centra skupień"}
plot(glass$RI, glass$Na, xlab = "RI", ylab = "Na",
     col = Glass$Type, pch = wyniki)
points(kmeans.k3$centers[,c("RI", "Na")], pch=16, cex=1.5, col=1:6)
```

Centra wywalone w kosmos, ale fajnie

```{r}
matrix <- table(wyniki, Glass$Type)

#Najlepsze dopasowanie
mapping <- solve_LSAP(matrix, maximum = TRUE)
wyniki <- mapping[wyniki]

matrix  <- table(wyniki, Glass$Type)
rownames(matrix ) <- as.character(c(1:3, 5:7))
colnames(matrix ) <- as.character(c(1:3, 5:7))

kable(matrix , caption = "Macierz błędów; metoda k-średnich")
```

Dokładność: `r round(sum(diag(matrix))/sum(matrix)*100, 2)`%.

### Partitioning Around Medoids (PAM)

```{r, fig.cap="coś"}
glass.conf_mat <- daisy(glass)

glass.pam7 <- pam(x=conf_mat, diss=TRUE, k=k)

wyniki <- glass.pam7$clustering

plot(pca_result$x[, 1], pca_result$x[, 2], xlab = xlab1, ylab = ylab1,
     col = wyniki, pch = as.numeric(Glass$Type))
```

Też słabo

```{r, fig.cap="coś, z medoidami"}
medoids <- glass[glass.pam7$id.med, ]

plot(glass$RI, glass$Na, xlab = "RI", ylab = "Na",
     col = wyniki, pch = as.numeric(Glass$Type))
points(medoids[,c("RI", "Na")], pch=16, cex=1.5, col=1:6)
```

Meh

```{r}
matrix <- table(wyniki, Glass$Type)

#Najlepsze dopasowanie
mapping <- solve_LSAP(matrix , maximum = TRUE)
wyniki <- mapping[wyniki]

conf_mat <- table(wyniki, Glass$Type)
rownames(matrix) <- as.character(c(1:3, 5:7))
colnames(matrix) <- as.character(c(1:3, 5:7))

kable(matrix, caption = "Macierz błędów; metoda k-średnich")
```

Dokładność: `r round(sum(diag(matrix))/sum(matrix)*100, 2)`%.

Gorzej niż k-średnie *shocked emoji*.

### Agglomerative Nesting (AGNES)

#### Najbliższy sąsiad

```{r, fig.cap="AGNES: single linkage"}
glass.agnes.single <- agnes(x = glass.conf_mat,
                            diss = TRUE,
                            method = "single")

fviz_dend(glass.agnes.single, cex=0.4, k = k)
```

```{r}
wyniki <- cutree(glass.agnes.single, k = k)

matrix <- table(wyniki, Glass$Type)

#Najlepsze dopasowanie
mapping <- solve_LSAP(matrix , maximum = TRUE)
wyniki <- mapping[wyniki]

conf_mat <- table(wyniki, Glass$Type)
rownames(matrix) <- as.character(c(1:3, 5:7))
colnames(matrix) <- as.character(c(1:3, 5:7))

kable(matrix, caption = "Macierz błędów; agnes, najbliższy sąsiad")
```

Dokładność: `r round(sum(diag(matrix))/sum(matrix)*100, 2)`%.

#### Najdalszy sąsiad

```{r, fig.cap="AGNES: complete linkage"}
glass.agnes.complete <- agnes(x = glass.conf_mat,
                            diss = TRUE,
                            method = "complete")

fviz_dend(glass.agnes.complete, cex=0.4, k = k)
```

```{r}
wyniki <- cutree(glass.agnes.complete, k = k)

matrix <- table(wyniki, Glass$Type)

#Najlepsze dopasowanie
mapping <- solve_LSAP(matrix , maximum = TRUE)
wyniki <- mapping[wyniki]

conf_mat <- table(wyniki, Glass$Type)
rownames(matrix) <- as.character(c(1:3, 5:7))
colnames(matrix) <- as.character(c(1:3, 5:7))

kable(matrix, caption = "Macierz błędów; agnes, najdalszy sąsiad")
```

Dokładność: `r round(sum(diag(matrix))/sum(matrix)*100, 2)`%.

#### Średnia odległość

```{r, fig.cap="AGNES: average linkage"}
glass.agnes.average <- agnes(x = glass.conf_mat,
                            diss = TRUE,
                            method = "average")

fviz_dend(glass.agnes.average, cex=0.4, k = k)
```

```{r}
wyniki <- cutree(glass.agnes.average, k = k)

matrix <- table(wyniki, Glass$Type)

#Najlepsze dopasowanie
mapping <- solve_LSAP(matrix , maximum = TRUE)
wyniki <- mapping[wyniki]

conf_mat <- table(wyniki, Glass$Type)
rownames(matrix) <- as.character(c(1:3, 5:7))
colnames(matrix) <- as.character(c(1:3, 5:7))

kable(matrix, caption = "Macierz błędów; agnes, średnia odległość")
```

Dokładność: `r round(sum(diag(matrix))/sum(matrix)*100, 2)`%.

### Divisive clustering (DIANA)

#### Odległość euklidesowa

```{r, fig.cap="AGNES: average linkage"}
glass.agnes.euclidean <- diana(x = glass.conf_mat,
                            diss = TRUE,
                            metric = "euclidean")

fviz_dend(glass.agnes.euclidean, cex=0.4, k = k)
```

```{r}
wyniki <- cutree(glass.agnes.euclidean, k = k)

matrix <- table(wyniki, Glass$Type)

#Najlepsze dopasowanie
mapping <- solve_LSAP(matrix , maximum = TRUE)
wyniki <- mapping[wyniki]

conf_mat <- table(wyniki, Glass$Type)
rownames(matrix) <- as.character(c(1:3, 5:7))
colnames(matrix) <- as.character(c(1:3, 5:7))

kable(matrix, caption = "Macierz błędów; agnes, średnia odległość")
```

Dokładność: `r round(sum(diag(matrix))/sum(matrix)*100, 2)`%.

#### Odległość Manhattan (taksówkowa)

```{r, fig.cap="AGNES: average linkage"}
glass.agnes.manhattan <- diana(x = glass.conf_mat,
                            diss = TRUE,
                            metric = "manhattan")

fviz_dend(glass.agnes.manhattan, cex=0.4, k = k)
```

```{r}
wyniki <- cutree(glass.agnes.manhattan, k = k)

matrix <- table(wyniki, Glass$Type)

#Najlepsze dopasowanie
mapping <- solve_LSAP(matrix , maximum = TRUE)
wyniki <- mapping[wyniki]

conf_mat <- table(wyniki, Glass$Type)
rownames(matrix) <- as.character(c(1:3, 5:7))
colnames(matrix) <- as.character(c(1:3, 5:7))

kable(matrix, caption = "Macierz błędów; agnes, średnia odległość")
```

Dokładność: `r round(sum(diag(matrix))/sum(matrix)*100, 2)`%.